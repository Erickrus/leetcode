{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.estimator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/leetcode/blob/master/tf_estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UH4XI2HST3jR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator\n"
      ]
    },
    {
      "metadata": {
        "id": "izyekzZWK3_7",
        "colab_type": "code",
        "outputId": "3ca4ebfa-d9ea-4a0f-9912-013c2b562ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "# prepare customized image data\n",
        "from PIL import Image\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "basePath = \"data\"\n",
        "def maybe_download(url, imageName):\n",
        "  try:\n",
        "    if not os.path.exists(imageName):\n",
        "      urlretrieve(url, imageName)\n",
        "      im = Image.open(imageName)\n",
        "      im = im.resize((28,28)).convert('L')\n",
        "      im.save(imageName, \"JPEG\")\n",
        "    else:\n",
        "      print(\"%s already exists\" % imageName)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def prepare_data():\n",
        "  maybe_download(\"https://media.newyorker.com/photos/5b48dfe812d15d2c9b41dbc7/master/w_649,c_limit/Rudell-RainbowBridge.jpg\",os.path.join(basePath, \"cat\",\"1.jpg\"))\n",
        "  maybe_download(\"http://www.pawschicago.org/fileadmin/user_upload/spay-young-pets.jpg\",os.path.join(basePath, \"cat\",\"2.jpg\"))\n",
        "  maybe_download(\"https://previews.123rf.com/images/nehru/nehru1505/nehru150501675/40068586-american-staffordshire-terrier-female-dog-image.jpg\", os.path.join(basePath, \"dog\",\"1.jpg\"))\n",
        "  maybe_download(\"https://previews.123rf.com/images/nehru/nehru1505/nehru150500704/39774720-american-staffordshire-terrier-female-dog-image.jpg\",os.path.join(basePath, \"dog\",\"2.jpg\"))\n",
        "\n",
        "def display_im(imageName):\n",
        "  im = Image.open(imageName)\n",
        "  imshow(im)  \n",
        "\n",
        "\n",
        "#!rm -Rf data\n",
        "!ls -al data\n",
        "!mkdir -p data/cat\n",
        "!mkdir -p data/dog\n",
        "prepare_data()\n",
        "#!apt install -y tree\n",
        "!tree data\n",
        "\n",
        "display_im('data/cat/1.jpg')\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 4 root root 4096 Mar 27 09:23 .\n",
            "drwxr-xr-x 1 root root 4096 Mar 27 09:23 ..\n",
            "drwxr-xr-x 2 root root 4096 Mar 27 09:23 cat\n",
            "drwxr-xr-x 2 root root 4096 Mar 27 09:23 dog\n",
            "data/cat/1.jpg already exists\n",
            "data/cat/2.jpg already exists\n",
            "data/dog/1.jpg already exists\n",
            "data/dog/2.jpg already exists\n",
            "data\n",
            "├── cat\n",
            "│   ├── 1.jpg\n",
            "│   └── 2.jpg\n",
            "└── dog\n",
            "    ├── 1.jpg\n",
            "    └── 2.jpg\n",
            "\n",
            "2 directories, 4 files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LZ1wSWCXDK95",
        "colab_type": "code",
        "outputId": "70750892-4ff4-4585-c004-df56892760dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "#loading the original dataset from mnist\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        " \n",
        "mnist=input_data.read_data_sets(\n",
        "    \"/data/machine_learning/mnist/\",\n",
        "    one_hot=False\n",
        ")\n",
        "\n",
        "print(mnist.train.labels.shape)\n",
        "print(type(mnist.train.labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /data/machine_learning/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting /data/machine_learning/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting /data/machine_learning/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /data/machine_learning/mnist/t10k-labels-idx1-ubyte.gz\n",
            "(55000,)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3zEQFG0IDjTn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义网络结构\n",
        "def neural_net(x_dict, params):\n",
        "  imData = x_dict['images']\n",
        "  layer_0 = tf.reshape(imData, [-1, 28*28])\n",
        "  layer_1 = tf.layers.dense(layer_0, params['n_hidden_1']) #全连接层\n",
        "  layer_2 = tf.layers.dense(layer_1, params['n_hidden_2']) #全连接层\n",
        "  \n",
        "  return tf.layers.dense(layer_2, params['num_classes']) #全连接层，输出层"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZKNoYXgDKVGd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "refer to: https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\n",
        "\n",
        "Depending on the value of mode, different arguments are required for EstimatorSpec\n",
        "\n",
        "    ModeKeys.TRAIN:   EstimatorSpec(mode, loss, train_op)\n",
        "    ModeKeys.EVAL:    EstimatorSpec(mode, loss)\n",
        "    ModeKeys.PREDICT: EstimatorSpec(mode, predictions)\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "0ErC4xAZDokg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    logits = neural_net(features, params) #输出\n",
        "    print(logits.shape)\n",
        "    \n",
        "    # EstimatorSpec.predictions\n",
        "    pred_classes = tf.argmax(logits, axis=1) #预测\n",
        "    \n",
        "    # 区分预测模式\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode,\n",
        "            predictions=pred_classes\n",
        "        )\n",
        "   \n",
        "    # EstimatorSpec.loss 定义损失和优化函数\n",
        "    loss_op=tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits,\n",
        "            labels=tf.cast(labels, dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    # EstimatorSpec.train_op\n",
        "    train_op = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate']).minimize(\n",
        "        loss_op,\n",
        "        global_step=tf.train.get_global_step()\n",
        "    ) \n",
        "    # EstimatorSpec.eval_metric_ops 精度\n",
        "    acc_op = tf.metrics.accuracy(\n",
        "        labels=labels,\n",
        "        predictions=pred_classes\n",
        "    )\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions=pred_classes,\n",
        "        loss=loss_op,\n",
        "        train_op=train_op,\n",
        "        eval_metric_ops={'accuracy': acc_op}\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_ARcmguOYBr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py\n",
        "\n",
        "params could be passed as a dict to the model_fn\n",
        "\n",
        "\n",
        "numpy_input_fn is defined: https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/inputs/numpy_io.py\n",
        "\n",
        "also: tf.data Dataset based input_fn: https://medium.com/tensorflow/multi-gpu-training-with-estimators-tf-keras-and-tf-data-ba584c3134db"
      ]
    },
    {
      "metadata": {
        "id": "GnRgx7fS6tbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3df920c1-f6bf-48f8-e92e-c9fa62585ca5"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "print(type(train_images))\n",
        "print(train_images.shape)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fq6vacyoX2O9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#自定义 input_fn\n",
        "# based on https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/inputs/numpy_io.py\n",
        "def udf_input_fn(\n",
        "    x, y=None, \n",
        "    batch_size=1, \n",
        "    num_epochs=1, \n",
        "    shuffle=None, \n",
        "    queue_capacity=4, \n",
        "    num_threads=1):\n",
        "  \n",
        "  def input_fn():\n",
        "    \n",
        "    features, target = [], np.zeros(len(x[\"images\"]),dtype=np.int32)\n",
        "    for i in range(x[\"images\"].shape[0]):\n",
        "    #for i in range(1):\n",
        "      im = Image.open(x[\"images\"][i])\n",
        "      # originally it should be divided by 255.0 only\n",
        "      # due to some network issues, it is further divided\n",
        "      im = np.asarray(im, dtype = np.float32) / 255.0 / 4.0\n",
        "      features.append(im)\n",
        "      if x[\"images\"][i].find('cat') >=0:\n",
        "        target[i] = 0\n",
        "      else:\n",
        "        target[i] = 1\n",
        "    \n",
        "    features = np.array(features)\n",
        "    target = np.array(target) \n",
        "    \n",
        "    features = {\"images\":tf.convert_to_tensor(features, dtype=tf.float32)}\n",
        "    # no need to convert it to one hot for sparse_softmax_cross_entropy_with_logits\n",
        "    target = tf.convert_to_tensor(target, dtype=tf.int32)\n",
        "    \n",
        "    return features, target\n",
        "      \n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h6TJF3SND5C7",
        "colab_type": "code",
        "outputId": "73335185-966f-4b48-91e7-7c6b8b512e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "cell_type": "code",
      "source": [
        "#参数设置\n",
        "num_steps=1000 #迭代次数\n",
        "batch_size=128 #批处理大小\n",
        "display_step=100 #输出间隔\n",
        "\n",
        "model = tf.estimator.Estimator(\n",
        "    model_fn,\n",
        "    params ={\n",
        "      'n_hidden_1': 256,    #第一个隐藏层神经元\n",
        "      'n_hidden_2': 256,    #第二个隐藏层神经元\n",
        "      'num_classes': 10,    #标签类别\n",
        "      'learning_rate': 0.1, #学习率\n",
        "    }\n",
        ")\n",
        "\n",
        "# suppose to change the control here\n",
        "usingMnistData = False\n",
        "if usingMnistData:\n",
        "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "      x={'images':mnist.train.images}, # numpy.ndarray\n",
        "      y=mnist.train.labels,            # numpy.ndarray\n",
        "      batch_size=batch_size,\n",
        "      num_epochs=None,\n",
        "      shuffle=True\n",
        "  )\n",
        "else:\n",
        "  # plugin the udf input fn\n",
        "  train_input_fn = udf_input_fn(\n",
        "    x={\n",
        "      \"images\":np.array([\n",
        "        'data/cat/1.jpg',\n",
        "        'data/cat/2.jpg',\n",
        "        'data/dog/1.jpg',\n",
        "        'data/dog/2.jpg'\n",
        "      ])\n",
        "    }, # numpy.ndarray\n",
        "    #y=xx,            # ignoring y, since y is generated from x\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True\n",
        "  )\n",
        "\n",
        "\n",
        "model.train(\n",
        "    train_input_fn,\n",
        "    steps=num_steps\n",
        ")\n",
        "\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images':mnist.test.images},\n",
        "    y=mnist.test.labels,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "e = model.evaluate(eval_input_fn)\n",
        "print(\"测试精度：\",e['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpax6_yeog\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpax6_yeog', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f00e24394a8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "(4, 10)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpax6_yeog/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.5165641, step = 1\n",
            "INFO:tensorflow:global_step/sec: 452.962\n",
            "INFO:tensorflow:loss = 0.003874624, step = 101 (0.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 504.992\n",
            "INFO:tensorflow:loss = 0.0015114056, step = 201 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.807\n",
            "INFO:tensorflow:loss = 0.00089304184, step = 301 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.928\n",
            "INFO:tensorflow:loss = 0.0006199628, step = 401 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 547.8\n",
            "INFO:tensorflow:loss = 0.00046912668, step = 501 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.412\n",
            "INFO:tensorflow:loss = 0.0003744849, step = 601 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 540.011\n",
            "INFO:tensorflow:loss = 0.00030992556, step = 701 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 539.669\n",
            "INFO:tensorflow:loss = 0.00026341755, step = 801 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 503.219\n",
            "INFO:tensorflow:loss = 0.00022831913, step = 901 (0.200 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpax6_yeog/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.00020132406.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "(?, 10)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-03-27T10:45:53Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpax6_yeog/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-03-27-10:45:53\n",
            "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.1138, global_step = 1000, loss = 15.103227\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmpax6_yeog/model.ckpt-1000\n",
            "测试精度： 0.1138\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}